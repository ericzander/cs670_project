{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: 'CS 670 Final Project: Steam Data Analysis'\n",
        "jupyter: python3\n",
        "format:\n",
        "  html:\n",
        "    code-fold: true\n",
        "    code-summary: \"Show code\"\n",
        "execute:\n",
        "  echo: true\n",
        "  eval: true\n",
        "  output: true\n",
        "---\n",
        "\n",
        "\n",
        "## Eric Zander\n",
        "\n",
        "---\n",
        "\n",
        "## Index\n",
        "\n",
        "1. [Dataset](#dataset)\n",
        "2. [Questions](#questions)\n",
        "3. [Setup](#setup)\n",
        "4. [Image Feature Extraction](#image-feature-extraction)\n",
        "5. [Tags](#tags)\n",
        "6. [Titles and Descriptions](#titles-and-descriptions)\n",
        "7. [Game Metadata](#game-metadata)\n",
        "8. [Concurrent Users](#concurrent-users)\n",
        "9. [Review Scores](#review-scores)\n",
        "10. [Discussion](#discussion)\n",
        "\n",
        "---\n",
        "\n",
        "## Dataset\n",
        "\n",
        "[Dataset GitHub Repo](https://github.com/NewbieIndieGameDev/steam-insights)\n",
        "\n",
        "### Description\n",
        "\n",
        "This dataset contains tables as CSV files describing video game data such as category, descriptions, game details, genres, promotional materials, reviews, tags, and other metadata for 140,082 games. These were sourced from the official [Steamworks Web APIs](https://partner.steamgames.com/doc/webapi) in Octobor 2024.\n",
        "\n",
        "Additionally, some data concerning predicted ownership, number of concurrent users, and more were sourced from the [SteamSpy](https://steamspy.com/) API.\n",
        "\n",
        "The data was reportedly collected for EDA and to explore patterns and opportunities in the game development space.\n",
        "\n",
        "### Discussion\n",
        "\n",
        "This data has a rich variety of data; promotional images, some textual metadata and reviews, release dates, prices, and more. However, there are some significant limitations as well. The numbers from the SteamSpy API in particular are predictive and some of the most interesting metrics (like number of owners and concurrent users) are imprecise. Also, this dataset being a snapshot from October 2024 limits the ability to answer many questions related to changes over time.\n",
        "\n",
        "### Summary\n",
        "\n",
        "The dataset is comprised of the following CSV files. Games are joined on universal \"app_id\" identifiers.\n",
        "\n",
        "* **categories.csv**: Misc. textual description tags (ex. multiplayer, family sharing)\n",
        "* **descriptions.csv**: Textual game descriptions from Steam pages.\n",
        "* **games.csv**: Metadata about the game such as title, release date and price.\n",
        "* **genres.csv**: One or more genres per game.\n",
        "* **promotional.csv**: Links to promotional material like screenshots and videos.\n",
        "* **reviews.csv**: Aggregations of user and critical review metrics. Textual critic reviews are included for some games.\n",
        "* **steamspy_insights.csv**: Various metrics sourcedd from the SteamSpy API.\n",
        "* **tags.csv**: Textual tags describing games. More specific than genre but less technical than categories.\n",
        "\n",
        "### Limitations\n",
        "\n",
        "Several steps were taken to further collect, clean, and wrangle the data outside of this notebook.\n",
        "\n",
        "Some genres were listed in different languages and had to be translated. This was limited enough to be reasonably done with a mapping data structure.\n",
        "\n",
        "Additionally, some prices had to be converted to one currency. The majority of games were listed in Euros and many more were in USD, GBP, or RUB; the latter were converted to the former and others were dropped during modeling with prices due to the risks posed by errors propogated by inaccurate exchange rates along with the relative infrequency and obscurity of games listed with different currencies. However, it should be noted that this choice may introduce bias related to such games.\n",
        "\n",
        "Some image feature extraction and text embedding work is done with game header images and game titles/descriptions. To limit the downloading and processing of over 100,000 games, only those predicted to have 10 or more concurrent users were processed in this manner. This leaves 6,174 games and limits the image and text embedding exploration to observing patterns in only more recently relevant games!\n",
        "\n",
        "Finally, another limitation to note is the lack of review score data for games without sufficient reviews; only those with 10 or more may be included in the predictions near the end.\n",
        "\n",
        "---\n",
        "\n",
        "## Questions\n",
        "\n",
        "* How do user and critic reviews relate to ownership?\n",
        "* Have prices shifted over the years?\n",
        "* Header Images\n",
        "    * What patterns may be observed promotional images?\n",
        "    * How do image features relate to other features (ex. tags, genres)?\n",
        "* Tags\n",
        "    * What tags co-occur?\n",
        "    * How do tags relate to metrics of success like concurrent users and reviews?\n",
        "* Success Metrics\n",
        "    * What factors seem to have a relationship with the number of concurrent users?\n",
        "    * What factors affect review scores?\n",
        "\n",
        "---\n",
        "\n",
        "## Setup\n",
        "\n",
        "***Imports***\n"
      ],
      "id": "085c656a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from collections import Counter\n",
        "from itertools import combinations\n",
        "import re\n",
        "\n",
        "######################\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import cv2\n",
        "\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "from scipy.spatial import cKDTree\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "from scipy.stats import linregress, pearsonr, spearmanr\n",
        "\n",
        "from scipy.cluster.hierarchy import linkage, dendrogram\n",
        "from scipy.spatial.distance import squareform\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "from sklearn.preprocessing import MultiLabelBinarizer, StandardScaler\n",
        "\n",
        "from sklearn.cross_decomposition import CCA\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.linear_model import ElasticNetCV\n",
        "from sklearn.feature_selection import SelectKBest, mutual_info_regression, VarianceThreshold\n",
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "import lightgbm as lgb\n",
        "import shap"
      ],
      "id": "eb687e0e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***Load Tabular Data***\n"
      ],
      "id": "56bfed14"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "data_dir = \"../data/oct24_clean/\"\n",
        "\n",
        "top_ids = pd.read_csv(data_dir + \"ge-10_concurrent-users.csv\")\n",
        "\n",
        "games = pd.read_csv(data_dir + \"games.csv\")\n",
        "desc = pd.read_csv(data_dir + \"descriptions.csv\")\n",
        "genres = pd.read_csv(data_dir + \"genres_translated.csv\")\n",
        "spy = pd.read_csv(data_dir + \"steamspy_insights.csv\") \n",
        "prom = pd.read_csv(data_dir + \"promotional.csv\")\n",
        "tags = pd.read_csv(data_dir + \"tags.csv\")\n",
        "reviews = pd.read_csv(data_dir + \"reviews.csv\")"
      ],
      "id": "50e58c22",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "----\n",
        "\n",
        "## Image Feature Extraction\n",
        "\n",
        "> Several features will be extracted for various explorations in the following sections. These include dominant colors via K-Means quantization, color histograms, proportions of signalling colors (ex. red, orange, yellow), and structural features with PCA.\n",
        "\n",
        "### Load Images\n"
      ],
      "id": "bad7b3ba"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class FeatureData:\n",
        "    def __init__(self, keys):\n",
        "        self.data = {}\n",
        "        self.map = {key: i for i, key in enumerate(keys)}\n",
        "\n",
        "    def add_column(self, column_name, values):\n",
        "        self.data[column_name] = values\n",
        "\n",
        "    def get_column(self, column_name):\n",
        "        if column_name not in self.data:\n",
        "            raise KeyError(f\"Column '{column_name}' not found.\")\n",
        "        return self.data[column_name]\n",
        "    \n",
        "    def __getitem__(self, column_name):\n",
        "        return self.get_column(column_name)\n",
        "\n",
        "    def get(self, col, id):\n",
        "        key = self.map[id]\n",
        "        return self.get_column(col)[self.map[id]]\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"{self.__class__.__name__}({self.data})\""
      ],
      "id": "f203a751",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def load_image(image_path):\n",
        "    image = cv2.imread(image_path)  # Loads image in BGR format\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
        "    return image\n",
        "\n",
        "image_dir = \"../data/img/header\"\n",
        "\n",
        "images = FeatureData(top_ids.app_id)\n",
        "\n",
        "img = []\n",
        "for app_id, idx in images.map.items():\n",
        "    img.append(load_image(f\"{image_dir}/{app_id}.jpg\"))\n",
        "\n",
        "images.add_column(\"img\", img)"
      ],
      "id": "922d1190",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def show_images(imgs):\n",
        "    fig, axes = plt.subplots(3, 2, figsize=(8, 6))\n",
        "    for i, ax in enumerate(axes.reshape(-1)):\n",
        "        ax.imshow(imgs[i])\n",
        "    plt.tight_layout()\n",
        "\n",
        "example_ids = [10, 1145360, 1657630, 362930, 322330, 1248130]\n",
        "\n",
        "# example_ids = np.random.choice(df.app_id, size=(6,))\n",
        "show_images([images.get(\"img\", id) for id in example_ids])"
      ],
      "id": "6cce2d67",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dominant Colors (K-Means Quantization)\n"
      ],
      "id": "f46366f6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def find_top_colors(image_array, k):\n",
        "    \"\"\"Find k dominant colors using MiniBatchKMeans for speed.\"\"\"\n",
        "    kmeans = MiniBatchKMeans(n_clusters=k, n_init=3, batch_size=1000)  \n",
        "    kmeans.fit(image_array)\n",
        "    return kmeans.cluster_centers_\n",
        "\n",
        "def replace_colors(image_array, top_colors):\n",
        "    \"\"\"Replace each pixel with the closest top color using cKDTree.\"\"\"\n",
        "    tree = cKDTree(top_colors)\n",
        "    _, indices = tree.query(image_array)  # Find closest colors\n",
        "    return top_colors[indices]\n",
        "\n",
        "def rgb_to_lab(image_array):\n",
        "    \"\"\"Convert RGB image to LAB color space for better clustering.\"\"\"\n",
        "    return cv2.cvtColor(image_array.astype(np.uint8), cv2.COLOR_RGB2LAB).reshape(-1, 3)\n",
        "\n",
        "def process_image(img, k):\n",
        "    \"\"\"Pipeline for a single image.\"\"\"\n",
        "    flat_lab = rgb_to_lab(img)\n",
        "    top_colors = find_top_colors(flat_lab, k)\n",
        "    return top_colors"
      ],
      "id": "a39e2deb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Process images in parallel\n",
        "k = 8\n",
        "knn_colors = Parallel(n_jobs=-1, backend=\"loky\")(\n",
        "    delayed(process_image)(img, k) for img in images[\"img\"])\n",
        "\n",
        "images.add_column(\"knn\", np.array(knn_colors))"
      ],
      "id": "80b2609a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "knn_images = []\n",
        "\n",
        "for img_idx in example_ids:\n",
        "    img = images.get(\"img\", img_idx)\n",
        "\n",
        "    flat_lab = rgb_to_lab(img).reshape(-1, 3)\n",
        "    top_colors = images.get(\"knn\", img_idx)\n",
        "\n",
        "    knn_image = replace_colors(flat_lab, top_colors)\n",
        "    knn_image = knn_image.reshape(img.shape).astype(np.uint8)\n",
        "    knn_image = cv2.cvtColor(knn_image, cv2.COLOR_LAB2RGB) # Back to RGB\n",
        "\n",
        "    knn_images.append(knn_image)\n",
        "\n",
        "print(f\"Quantization: k={k}\")\n",
        "show_images(knn_images)"
      ],
      "id": "3dc541fc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Color Distribution (Histogram Analysis)\n"
      ],
      "id": "1a95cbb3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Compute color histograms for all images in the dataset\n",
        "color_histograms = [[] for _ in range(3)]\n",
        "\n",
        "for img in images[\"img\"]:\n",
        "    # Split channels\n",
        "    r_hist = cv2.calcHist([img], [2], None, [256], [0, 256]).flatten()\n",
        "    g_hist = cv2.calcHist([img], [1], None, [256], [0, 256]).flatten()\n",
        "    b_hist = cv2.calcHist([img], [0], None, [256], [0, 256]).flatten()\n",
        "    \n",
        "    # Normalize histograms (optional)\n",
        "    r_hist /= r_hist.sum()\n",
        "    g_hist /= g_hist.sum()\n",
        "    b_hist /= b_hist.sum()\n",
        "    \n",
        "    # Store data\n",
        "    color_histograms[0].append(r_hist)\n",
        "    color_histograms[1].append(g_hist)\n",
        "    color_histograms[2].append(b_hist)\n",
        "\n",
        "# Convert lists to NumPy arrays\n",
        "for i in range(3):\n",
        "    color_histograms[i] = np.array(color_histograms[i])\n",
        "color_histograms = np.array(color_histograms)\n",
        "color_histograms = np.transpose(color_histograms, axes=[1, 0, 2])\n",
        "\n",
        "images.add_column(\"hist\", color_histograms)"
      ],
      "id": "6109d46e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "img_idx = 2\n",
        "\n",
        "y = color_histograms[img_idx]\n",
        "x = np.arange(y.shape[1])\n",
        "y_tot = y.sum(axis=0)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10, 3.5))\n",
        "\n",
        "axes[0].fill_between(x, y_tot, color=\"gray\", alpha=0.3, label=\"Total\")\n",
        "axes[0].fill_between(x, y[0], color=\"red\", alpha=0.5, label=\"R\")\n",
        "axes[0].fill_between(x, y[1], color=\"green\", alpha=0.5, label=\"G\")\n",
        "axes[0].fill_between(x, y[2], color=\"blue\", alpha=0.5, label=\"B\")\n",
        "axes[0].set_title(\"Color Histogram\")\n",
        "\n",
        "axes[1].imshow(images[\"img\"][img_idx])\n",
        "\n",
        "plt.show()"
      ],
      "id": "ff709078",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Structural Features (PCA)\n"
      ],
      "id": "481cf94a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "EIG_SIZE = (128, 128)\n",
        "EIG_COMPONENTS = 20\n",
        "\n",
        "# Convert images to grayscale and flatten\n",
        "eig = []\n",
        "\n",
        "for img in images[\"img\"]:\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)  # Convert to grayscale\n",
        "    resized = cv2.resize(gray, EIG_SIZE)          # Resize to fixed size\n",
        "    flattened = resized.flatten()                 # Flatten into 1D vector\n",
        "    eig.append(flattened)\n",
        "\n",
        "# Stack all images into a matrix (each row is an image)\n",
        "image_matrix = np.stack(eig)\n",
        "\n",
        "# Apply PCA\n",
        "pca = PCA(n_components=EIG_COMPONENTS)\n",
        "pca.fit(image_matrix)\n",
        "\n",
        "# Visualize the top principal components\n",
        "fig, axes = plt.subplots(EIG_COMPONENTS // 5, 5, figsize=(10, 8))\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    if i >= EIG_COMPONENTS:\n",
        "        break\n",
        "    eigenimage = pca.components_[i].reshape(EIG_SIZE)  # Reshape back to image\n",
        "    ax.imshow(eigenimage, cmap='gray')\n",
        "    ax.set_title(f'PC {i+1}')\n",
        "    ax.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Project images onto principal components\n",
        "pca_features = pca.transform(image_matrix)\n",
        "\n",
        "images.add_column(\"pca\", pca_features)"
      ],
      "id": "97d3f767",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Signalling Colors\n"
      ],
      "id": "fc0e81b6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def extract_signaling_color_proportion(color_histograms):\n",
        "    \"\"\"Extract proportion of signaling colors (bright red, yellow, orange) from histograms.\"\"\"\n",
        "    signaling_proportions = []\n",
        "\n",
        "    for hist in color_histograms:\n",
        "        red_hist, green_hist, blue_hist = hist  # Each image's RGB histograms\n",
        "\n",
        "        # Define bright red & yellow regions in RGB histograms (sums are # of pixels in that range for that histogram)\n",
        "        bright_red = np.sum(red_hist[200:256]) - np.sum(green_hist[200:256]) - np.sum(blue_hist[200:256])\n",
        "        bright_yellow = np.sum(red_hist[200:256]) + np.sum(green_hist[200:256]) - np.sum(blue_hist[200:256])\n",
        "        bright_orange = np.sum(red_hist[180:220]) + np.sum(green_hist[100:180]) - np.sum(blue_hist[0:100])\n",
        "\n",
        "        # Normalize by total pixels to get proportion\n",
        "        total_pixels = np.sum(red_hist) + np.sum(green_hist) + np.sum(blue_hist)\n",
        "        signaling_score = (bright_red + bright_yellow + bright_orange) / total_pixels\n",
        "\n",
        "        signaling_proportions.append(max(0, signaling_score))  # Ensure non-negative values\n",
        "\n",
        "    return np.array(signaling_proportions)\n",
        "\n",
        "# Extract signaling color proportions\n",
        "signaling_scores = extract_signaling_color_proportion(color_histograms)\n",
        "\n",
        "images.add_column(\"signal\", signaling_scores)"
      ],
      "id": "11f0e024",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def highlight_signaling_colors(image):\n",
        "    hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
        "\n",
        "    # Define color ranges\n",
        "    red_lower1, red_upper1 = np.array([0, 120, 120]), np.array([10, 255, 255])   # Red (low end)\n",
        "    red_lower2, red_upper2 = np.array([170, 120, 120]), np.array([180, 255, 255]) # Red (high end)\n",
        "    yellow_lower, yellow_upper = np.array([20, 100, 100]), np.array([35, 255, 255]) # Yellow\n",
        "    orange_lower, orange_upper = np.array([10, 150, 100]), np.array([20, 255, 255]) # Orange\n",
        "\n",
        "    # Create masks for each color\n",
        "    mask_red = cv2.inRange(hsv, red_lower1, red_upper1) | cv2.inRange(hsv, red_lower2, red_upper2)\n",
        "    mask_yellow = cv2.inRange(hsv, yellow_lower, yellow_upper)\n",
        "    mask_orange = cv2.inRange(hsv, orange_lower, orange_upper)\n",
        "\n",
        "    # Combine masks\n",
        "    mask = mask_red | mask_yellow | mask_orange\n",
        "\n",
        "    # Convert mask to 3-channel (Red overlay)\n",
        "    mask_colored = np.zeros_like(image)  # Start with a black image\n",
        "    mask_colored[:, :, 0] = mask  # Assign mask to Red channel (0: Red, 1: Green, 2: Blue)\n",
        "\n",
        "    # Overlay mask on the original image\n",
        "    overlayed = cv2.addWeighted(image, 0.7, mask_colored, 0.5, 0)\n",
        "\n",
        "    return overlayed\n",
        "\n",
        "image = images[\"img\"][2]\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "\n",
        "# Apply the overlay function\n",
        "highlighted_image = highlight_signaling_colors(image)\n",
        "\n",
        "# Show the result\n",
        "axes[0].imshow(image)\n",
        "axes[1].imshow(highlighted_image)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Signaling Colors Highlighted\")\n",
        "plt.show()"
      ],
      "id": "ff486753",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.figure(figsize=(8, 5))\n",
        "\n",
        "sns.histplot(signaling_scores, bins=50, edgecolor=\"black\", alpha=0.7, color=\"orange\")\n",
        "plt.xlabel(\"Signaling Color Proportion\")\n",
        "plt.ylabel(\"log(Frequency)\")\n",
        "plt.title(\"Signaling Color Proportions Distribution (Logarithmic)\")\n",
        "plt.yscale(\"log\")"
      ],
      "id": "7c42060a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> The distribution of signaling color proportion is notable for its skew; most images are not dominated by red, orange, and yellow. It could be that the presence of signaling colors is more important than proportion though; a binary feature variable will be created for use in modeling based on thresholding.\n"
      ],
      "id": "5afed1e8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "images.add_column(\"signal_present\", (signaling_scores > 0.1).astype(int))\n",
        "\n",
        "signal_df = pd.DataFrame(\n",
        "    {\"signal_proportion\": images.data[\"signal\"], \"signal_present\": images.data[\"signal_present\"]},\n",
        "    index=images.map.keys())\n",
        "signal_df.index\n",
        "signal_df"
      ],
      "id": "0dcebc96",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create DataFrames\n"
      ],
      "id": "b759ef0f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Map histogram features to RGB labels\n",
        "def interpret_hist_feature(hist_index):\n",
        "    channel = hist_index // 256  # 0,1,2 -> RGB\n",
        "    bin_index = hist_index % 256\n",
        "    channels = [\"red\", \"green\", \"blue\"]\n",
        "    return f\"{channels[channel]}_{bin_index}\"\n",
        "\n",
        "hist_flat = images[\"hist\"].reshape(6174, -1)  # Shape becomes (6174, 768)\n",
        "hist_df = pd.DataFrame(\n",
        "    hist_flat, index=list(images.map.keys()),\n",
        "    columns=[f\"hist_{i}\" for i in range(hist_flat.shape[1])])\n",
        "\n",
        "# Convert PCA features to dataframe\n",
        "pca_df = pd.DataFrame(\n",
        "    images[\"pca\"], index=list(images.map.keys()),\n",
        "    columns=[f\"pca_{i}\" for i in range(images[\"pca\"].shape[1])])\n",
        "\n",
        "# Rename histogram feature columns\n",
        "hist_feature_names = [interpret_hist_feature(i) for i in range(768)]\n",
        "hist_df.columns = hist_feature_names\n",
        "\n",
        "# Get projected histograms\n",
        "pca_color = PCA(n_components=0.95)\n",
        "hist_pca_df = pd.DataFrame(pca_color.fit_transform(hist_df), index=hist_df.index)\n",
        "hist_pca_df.columns = [f\"hist{i}\" for i in hist_pca_df.columns]\n",
        "\n",
        "# Concatenate color histogram, PCA features, and signaling color features\n",
        "visual_features_df = pd.concat([hist_pca_df, pca_df, signal_df], axis=1)\n",
        "\n",
        "# Normalize visual features\n",
        "# scaler = StandardScaler()\n",
        "# visual_features_scaled = scaler.fit_transform(visual_features_df)\n",
        "# visual_features_df = pd.DataFrame(visual_features_scaled, index=visual_features_df.index, columns=visual_features_df.columns)"
      ],
      "id": "3ed0f03d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Tags\n",
        "\n",
        "> Tags provide finer-grained descriptions of games than genre. Here, they will be encoded for modeling later and explored for aspects such as co-occurrence and relationships to image features.\n",
        "\n",
        "### Encoding\n"
      ],
      "id": "5d3a840e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Group tags by app_id\n",
        "grouped_df = tags.groupby(\"app_id\")[\"tag\"].apply(list).reset_index()\n",
        "\n",
        "# Apply MultiLabelBinarizer\n",
        "mlb = MultiLabelBinarizer()\n",
        "tag_matrix = mlb.fit_transform(grouped_df[\"tag\"])\n",
        "\n",
        "# Convert to DataFrame\n",
        "tag_df = pd.DataFrame(tag_matrix, columns=mlb.classes_, index=grouped_df[\"app_id\"])"
      ],
      "id": "7b97ec38",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Top Tag Co-Occurence\n"
      ],
      "id": "3a751c6e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "top_n = 25\n",
        "tag_counts = tags[\"tag\"].value_counts()\n",
        "top_tags = tag_counts.nlargest(top_n).index\n",
        "\n",
        "# Filter dataset to include only top tags\n",
        "filtered_df = tags[tags[\"tag\"].isin(top_tags)]\n",
        "\n",
        "# Compute matrix\n",
        "co_mat = pd.crosstab(filtered_df[\"app_id\"], filtered_df[\"tag\"])\n",
        "co_mat = co_mat.T.dot(co_mat)\n",
        "\n",
        "# Normalize\n",
        "co_mat = co_mat / co_mat.max().max()\n",
        "\n",
        "# Zero out diagonal\n",
        "np.fill_diagonal(co_mat.values, 0.0)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(co_mat, cmap=\"coolwarm\", annot=False, xticklabels=True, yticklabels=True)\n",
        "plt.title(\"Tag Co-Occurrence Heatmap (Top {} Tags)\".format(top_n))\n",
        "plt.xlabel(\"Tags\")\n",
        "plt.ylabel(\"Tags\")\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0)\n",
        "plt.show()"
      ],
      "id": "80e65544",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Convert similarity to distance\n",
        "distance_matrix = 1 - co_mat\n",
        "\n",
        "# Convert to condensed format (required for linkage)\n",
        "condensed_dist = squareform(distance_matrix, checks=False)\n",
        "\n",
        "# Perform clustering\n",
        "linkage_matrix = linkage(condensed_dist, method='ward')\n",
        "\n",
        "plt.figure(figsize=(3, 8))\n",
        "threshold = 1.0\n",
        "dendrogram(linkage_matrix, labels=co_mat.index, leaf_rotation=0, orientation=\"right\", color_threshold=threshold)\n",
        "plt.axvline(threshold, color=\"gray\", ls=\":\", linewidth=1)\n",
        "plt.title(\"Tag Clustering Dendrogram (Ward Linkage)\", y=1.02)\n",
        "plt.xlabel(\"Distance\")\n",
        "plt.show()"
      ],
      "id": "073546c6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tag Combinations\n"
      ],
      "id": "65ae2a69"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Generate tag pairs\n",
        "tag_pairs = []\n",
        "for tags in grouped_df[\"tag\"]:\n",
        "    tag_pairs.extend(combinations(tags, 2))\n",
        "\n",
        "# Count frequency of each tag pair\n",
        "tag_pair_counts = Counter(tag_pairs)\n",
        "\n",
        "# Convert to DataFrame\n",
        "tag_pair_df = pd.DataFrame(tag_pair_counts.items(), columns=[\"TagPair\", \"Count\"]).sort_values(by=\"Count\", ascending=False)"
      ],
      "id": "c281bc19",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Define tags to exclude\n",
        "exclude_tags = {\"Singleplayer\", \"2D\", \"3D\", \"Multiplayer\"}\n",
        "\n",
        "# Get the top 30 tag pairs\n",
        "df_top = tag_pair_df.nlargest(30, \"Count\")\n",
        "df_top[\"TagPair\"] = df_top[\"TagPair\"].astype(str)\n",
        "\n",
        "# Filter out tag pairs containing excluded tags\n",
        "df_filtered = tag_pair_df[~tag_pair_df[\"TagPair\"].apply(lambda x: any(tag in x for tag in exclude_tags))]\n",
        "\n",
        "# Ensure the filtered dataset also contains 30 entries\n",
        "df_filtered_top = df_filtered.nlargest(30, \"Count\")\n",
        "df_filtered_top[\"TagPair\"] = df_filtered_top[\"TagPair\"].astype(str)\n",
        "\n",
        "# Create subplots\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 7), sharex=True)\n",
        "\n",
        "# Plot all top tag pairs\n",
        "sns.barplot(y=df_top[\"TagPair\"], x=df_top[\"Count\"], hue=df_top[\"Count\"], palette=\"viridis\", ax=axes[0])\n",
        "axes[0].set_xlabel(\"Count\")\n",
        "axes[0].set_ylabel(\"Tag Pair\")\n",
        "axes[0].set_title(\"Top 30 Most Common Tag Pairs\")\n",
        "axes[0].grid(axis=\"x\")\n",
        "axes[0].set_axisbelow(True)\n",
        "\n",
        "# Plot filtered tag pairs (ensuring it also contains 30 entries)\n",
        "sns.barplot(y=df_filtered_top[\"TagPair\"], x=df_filtered_top[\"Count\"],\n",
        "            hue=df_filtered_top[\"Count\"], palette=\"viridis\", ax=axes[1])\n",
        "axes[1].set_xlabel(\"Count\")\n",
        "axes[1].set_ylabel(None)\n",
        "axes[1].set_title(\"Top 30 Tag Pairs\\n(Excluding Singleplayer/Multiplayer, 2D/3D\")\n",
        "axes[1].grid(axis=\"x\")\n",
        "axes[1].set_axisbelow(True)\n",
        "axes[1].legend(loc=\"lower right\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "6d727319",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Filter rare tag pairs\n",
        "rare_pairs = [pair for pair, count in tag_pair_counts.items() if count <= 1]\n",
        "\n",
        "# Extract individual tags from rare pairs\n",
        "uncommon_tags = [tag for pair in rare_pairs for tag in pair]\n",
        "\n",
        "# Count occurrences of each tag in these rare pairs\n",
        "uncommon_tag_counts = Counter(uncommon_tags)\n",
        "\n",
        "uncommon_tag_df = pd.DataFrame(\n",
        "    uncommon_tag_counts.items(),\n",
        "    columns=[\"Tag\", \"Count\"]).sort_values(by=\"Count\", ascending=False)\n",
        "\n",
        "df = uncommon_tag_df.nlargest(30, \"Count\")\n",
        "\n",
        "plt.figure(figsize=(6, 7))\n",
        "sns.barplot(y=df[\"Tag\"], x=df[\"Count\"], hue=df[\"Count\"], palette=\"rocket\", legend=False)\n",
        "plt.xlabel(\"Count\")\n",
        "plt.ylabel(\"Tag Pair\")\n",
        "plt.title(\"Tags by # of Appearances in Unique Pairings\")\n",
        "plt.gca().set_axisbelow(True)\n",
        "plt.grid(axis=\"x\")\n",
        "plt.show()"
      ],
      "id": "12827c97",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Relationship to Image Features\n"
      ],
      "id": "79edc40c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Count how many games have each tag and keep only frequent\n",
        "min_games = 50\n",
        "tag_counts = tag_df.sum()\n",
        "common_tags = tag_counts[tag_counts >= min_games].index\n",
        "\n",
        "# Filter tag matrix to only include common tags\n",
        "filtered_tag_df = tag_df[common_tags]\n",
        "\n",
        "# Compute correlation matrix again with meaningful labels\n",
        "combined_df = filtered_tag_df.merge(hist_df, left_index=True, right_index=True).merge(pca_df, left_index=True, right_index=True)\n",
        "correlation_matrix = combined_df.corr(\"spearman\")\n",
        "\n",
        "# Extract correlations of tags with image features\n",
        "tag_image_corr = correlation_matrix.loc[filtered_tag_df.columns, hist_feature_names + list(pca_df.columns)]\n",
        "\n",
        "# Show top readable correlations\n",
        "top_tag_correlations = tag_image_corr.unstack().sort_values(key=abs, ascending=False).head(30)\n",
        "\n",
        "df = top_tag_correlations.rename_axis(['Feature', 'Tag']).reset_index(name='Corr')\n",
        "pivot = df.pivot(index=\"Feature\", columns=\"Tag\", values=\"Corr\")\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(pivot, annot=True, cmap=\"coolwarm\", center=0)\n",
        "plt.title(\"Top Feature/Tag Correlations (Spearman's Rank)\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "6fb3533d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Looking at correlations between colors and tags, it seems that images with high blue values seem to correspond seem to be relatiely commonplace among games tagged as 'Anime' and 'Cute'. Additionally, the first pricinipal component seems to have some utility at differentiated between 'Cute' and 'Horror' games. However, that the highest correlations hover around 0.25 suggests that tags and images aren't reliably related in a way that is all that significant. This can be further seen by looking at the distribution.\n"
      ],
      "id": "e808cbf1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Flatten correlation values into a list for distribution analysis\n",
        "corr_values = tag_image_corr.values.flatten()\n",
        "\n",
        "corr_values = corr_values[~pd.isna(corr_values)]\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.histplot(corr_values, bins=50, kde=True, color=\"royalblue\")\n",
        "plt.xlabel(\"Spearman Correlation\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.title(\"Distribution of Tag-Image Feature Correlations\")\n",
        "plt.axvline(x=corr_values.mean(), color=\"red\", linestyle=\"dashed\", label=f\"Mean: {corr_values.mean():.3f}\")\n",
        "plt.legend()\n",
        "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
        "plt.show()"
      ],
      "id": "eaf9b6f5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> The distribution of correlations is a rather unremarkable standard distribution; the magnitudes are low and the mean is practically zero.\n",
        "\n",
        "---\n",
        "\n",
        "## Titles and Descriptions\n",
        "\n",
        "> To make some use of titles and descriptions, embeddings can be created. There are many options here, but simplicity\n",
        "\n",
        "### Embedding (w/ Sentence Transformers, ie SBERT)\n"
      ],
      "id": "5574a6f8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "top_games = top_ids.merge(games, how=\"left\", on=\"app_id\")\n",
        "top_desc = top_ids.merge(desc, how=\"left\", on=\"app_id\")"
      ],
      "id": "9e5cdd7f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load model\n",
        "# model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\", device=\"cpu\")\n",
        "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L12-v2\", device=\"cpu\")\n",
        "\n",
        "# Encode text\n",
        "title_emb = model.encode(top_games.name, device=\"cpu\")\n",
        "desc_emb = model.encode(top_desc.summary, device=\"cpu\")"
      ],
      "id": "66703195",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create column names\n",
        "title_columns = [f\"title_{i}\" for i in range(384)]\n",
        "description_columns = [f\"description_{i}\" for i in range(384)]\n",
        "\n",
        "embedding_df = pd.DataFrame(\n",
        "    np.hstack((title_emb, desc_emb)),\n",
        "    index=top_games.app_id,\n",
        "    columns=title_columns + description_columns \n",
        ")\n",
        "embedding_df = embedding_df.reset_index()\n",
        "embedding_df"
      ],
      "id": "caf4b535",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "reducer = TSNE(n_components=2, perplexity=30, random_state=42)\n",
        "embeddings_2d = reducer.fit_transform(desc_emb)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], s=5, alpha=0.5)\n",
        "plt.xlabel(\"t-SNE Dimension 1\")\n",
        "plt.ylabel(\"t-SNE Dimension 2\")\n",
        "plt.title(\"Steam Game Embeddings (2D t-SNE Projection)\")\n",
        "plt.show()"
      ],
      "id": "cf7a0047",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Correlation with Image Features\n"
      ],
      "id": "fa2dbe50"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Canonical correlation (corr between projections of visual features and description embeddings)\n",
        "cca = CCA(n_components=1)\n",
        "X_c, Y_c = cca.fit_transform(visual_features_df.values, desc_emb)\n",
        "corr = np.corrcoef(X_c.T, Y_c.T)[0, 1]\n",
        "print(f\"CCA correlation: {corr:.3f}\")"
      ],
      "id": "a5966a61",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> There seems to be moderate correlation between image features and description embeddings. This is interesting, but also hints at some possible redundancy if modeling with both as predictors.\n",
        "\n",
        "---\n",
        "\n",
        "## Game Metadata\n",
        "\n",
        "#### Wrangling and Cleaning\n",
        "\n",
        "***Dates***\n"
      ],
      "id": "9ae1832a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "games_df = games\n",
        "\n",
        "games_df[\"release_date\"] = pd.to_datetime(games_df[\"release_date\"], errors=\"coerce\")\n",
        "games_df[\"date_missing\"] = games_df[\"release_date\"].isna().astype(int)\n",
        "\n",
        "games_df[\"release_year\"] = games_df[\"release_date\"].dt.year.fillna(-1).astype(int)\n",
        "games_df[\"release_month\"] = games_df[\"release_date\"].dt.month.fillna(-1).astype(int)\n",
        "games_df[\"release_day\"] = games_df[\"release_date\"].dt.day.fillna(-1).astype(int)"
      ],
      "id": "73a3aa97",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***Price (Convert to Euros)***\n"
      ],
      "id": "1d5f1762"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Safe regex extraction function\n",
        "def extract_price_currency(price_str, is_free):\n",
        "    if is_free == 1 or pd.isna(price_str):\n",
        "        return 0.0, \"FREE\"\n",
        "    \n",
        "    price_match = re.search(r\"'final':\\s*(\\d+)\", price_str)\n",
        "    currency_match = re.search(r\"'currency':\\s*'(\\w+)'\", price_str)\n",
        "\n",
        "    price = int(price_match.group(1)) / 100 if price_match else 0.0  # Convert cents to float\n",
        "    currency = currency_match.group(1) if currency_match else \"UNKNOWN\"\n",
        "\n",
        "    return price, currency\n",
        "\n",
        "# Apply function to extract price and currency\n",
        "games_df[['price', 'currency']] = games_df.apply(\n",
        "    lambda row: pd.Series(extract_price_currency(row[\"price_overview\"], row[\"is_free\"])), axis=1)\n",
        "\n",
        "# October 2024\n",
        "EXCHANGE_RATES = {\n",
        "    \"USD\": 1/1.0882,  # https://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=OJ:C_202405911\n",
        "    \"GBP\": 1/0.83753, # https://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=OJ:C_202405911\n",
        "    \"RUB\": 0.009545   # https://www.x-rates.com/average/?from=RUB&to=EUR&amount=1&year=2024\n",
        "}\n",
        "\n",
        "def convert_price(row):\n",
        "    if row[\"currency\"] == \"FREE\":\n",
        "        return 0.0\n",
        "    elif row[\"currency\"] in EXCHANGE_RATES:\n",
        "        return row[\"price\"] * EXCHANGE_RATES[row[\"currency\"]]\n",
        "    return row[\"price\"]  # Keep EUR prices unchanged\n",
        "\n",
        "# Apply conversion\n",
        "games_df[\"price_euros\"] = games_df.apply(convert_price, axis=1)\n",
        "\n",
        "# Filter dataset to keep only EUR, FREE, and converted currencies\n",
        "valid_currencies = {\"EUR\", \"FREE\"}\n",
        "games_df = games_df[games_df[\"currency\"].isin(valid_currencies | set(EXCHANGE_RATES.keys()))]\n",
        "\n",
        "games_df = games_df.drop(columns=[\"currency\", \"price\"])"
      ],
      "id": "ae44265d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Concurrent Users\n",
        "\n",
        "> To see if these selected and derived features are indicative (or at least related) to a game's success, some visualizations and attempts at modeling the number of concurrent users is included. As a note, the focus on this metric stems from limitations with another; while the SteamSpy portion of the dataset does offer an estimated number of owners, these are very coarsely binned and possibly less relevant at present due to the inclusion of many older games.\n",
        "\n",
        "### Distribution\n"
      ],
      "id": "938f043a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "y = top_ids.merge(spy, on=\"app_id\")[\"concurrent_users_yesterday\"]\n",
        "\n",
        "fig, axes = plt.subplots(2, 1, figsize=(7, 5))\n",
        "\n",
        "# Original scale\n",
        "sns.histplot(y, bins=50, kde=True, ax=axes[0])\n",
        "axes[0].set_title(\"Original Concurrent Users Distribution\")\n",
        "axes[0].set_xlabel(\"Concurrent Users\")\n",
        "axes[0].set_ylabel(\"Frequency\")\n",
        "\n",
        "# Log-transformed\n",
        "sns.histplot(np.log1p(y), bins=50, kde=True, color=\"orange\", ax=axes[1])\n",
        "axes[1].set_title(\"Log-Transformed Concurrent Users\")\n",
        "axes[1].set_xlabel(\"log1p(Concurrent Users)\")\n",
        "axes[1].set_ylabel(\"Frequency\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "c719bd6c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> The distribution of concurrent users is highly skewed; a small proportion of games get a high proportion of players. Modeling will likely benefit from log transformation.\n",
        "\n",
        "### Relationship to Image Features\n"
      ],
      "id": "b2af2c48"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def plot_histogram_tsne_users(user_counts):\n",
        "    \"\"\"t-SNE visualization of color histograms with color mapping based user count\"\"\"\n",
        "    hist_vectors = []\n",
        "    user_values = []\n",
        "\n",
        "    for app_id in images.map.keys():\n",
        "        hist = images.get(\"hist\", app_id).flatten()  # Flatten histogram vector\n",
        "        \n",
        "        if app_id in user_counts:\n",
        "            hist_vectors.append(hist)\n",
        "            user_values.append(user_counts[app_id])\n",
        "\n",
        "    hist_vectors = np.array(hist_vectors)\n",
        "    user_values = np.array(user_values)\n",
        "\n",
        "    # Reduce to 2D using t-SNE\n",
        "    tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
        "    embeddings = tsne.fit_transform(hist_vectors)\n",
        "\n",
        "    df = pd.DataFrame({\n",
        "        \"x\": embeddings[:, 0],\n",
        "        \"y\": embeddings[:, 1],\n",
        "        \"users\": user_values,\n",
        "    })\n",
        "\n",
        "    # Scatter plot with color gradient based on users\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    scatter = plt.scatter(df[\"x\"], df[\"y\"], c=df[\"users\"], cmap=\"viridis\", alpha=0.7)\n",
        "    plt.colorbar(scatter, label=\"log(Concurrent Users + 1)\")\n",
        "    plt.title(\"t-SNE Visualization of Color Histograms Colored by log(Concurrent Users + 1)\")\n",
        "    plt.xlabel(\"t-SNE Component 1\")\n",
        "    plt.ylabel(\"t-SNE Component 2\")\n",
        "    plt.show()\n",
        "\n",
        "# Convert log user counts to a dictionary\n",
        "user_counts = spy.groupby(\"app_id\")[\"concurrent_users_yesterday\"].apply(lambda x: np.log1p(x).iloc[0]).to_dict()\n",
        "\n",
        "plot_histogram_tsne_users(user_counts)"
      ],
      "id": "1bb0e855",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "user_values = np.array([user_counts.get(app_id, 0) for app_id in images.map.keys()])  # Match order\n",
        "\n",
        "df = pd.DataFrame({\"signaling\": signaling_scores, \"log_signaling\": np.log1p(signaling_scores),\n",
        "                   \"log_users\": np.log1p(user_values)})\n",
        "\n",
        "# Compute regression \n",
        "slope_raw, intercept_raw, r_value_raw, p_value_raw, std_err_raw = linregress(df[\"log_signaling\"], df[\"log_users\"])\n",
        "x_range = np.linspace(df[\"log_signaling\"].min(), df[\"log_signaling\"].max(), 100)\n",
        "y_fit_raw = slope_raw * x_range + intercept_raw\n",
        "\n",
        "# Compute Pearson correlation\n",
        "pearson_corr, pearson_p = pearsonr(df[\"log_signaling\"], df[\"log_users\"])\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.scatter(df[\"log_signaling\"], df[\"log_users\"], alpha=0.5, color=\"blue\", label=\"Raw Data\")\n",
        "plt.plot(x_range, y_fit_raw, color=\"red\", linestyle=\"--\", label=f\"Regression (R={r_value_raw:.2f})\")\n",
        "plt.xlabel(\"log(Signaling Color Proportion + 1)\")\n",
        "plt.ylabel(\"log(Concurrent Users + 1)\")\n",
        "plt.title(f\"Scatterplot of Signaling Colors vs. Users\\n Pearson R = {pearson_corr:.2f}, p = {pearson_p:.4f}\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "id": "711399b9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Based on these plots, there doesn't appear to be evidence that either color histograms or signaling color proportions are meaningfully connected to concurrent users.\n",
        "\n",
        "### Prediction\n",
        "\n",
        "> Along with the generally high dimensionality of the data, the lack of a clear relationship between image features and concurrent users indicates the usefulness of a feature selection efforts. After experimenting, gradient boosting models provided the best results. To offset their relative lack of explainability, Shapley Additive Explanations (SHAP) values have been computed and displayed.\n",
        "\n",
        "***Prepare Data***\n"
      ],
      "id": "e755a58b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "features = [\"app_id\", \"price_euros\", \"date_missing\", \"release_year\", \"release_month\", \"release_day\"]\n",
        "\n",
        "pred_df = top_ids.merge(games_df, how=\"left\", on=\"app_id\")\n",
        "\n",
        "pred_df = pred_df.loc[:, features]\n",
        "\n",
        "pred_df = spy.loc[:, [\"app_id\", \"concurrent_users_yesterday\"]].merge(pred_df, on=\"app_id\", how=\"right\")\n",
        "\n",
        "pred_df = pred_df.merge(visual_features_df.reset_index().rename({\"index\": \"app_id\"}, axis=1),\n",
        "              how=\"left\", on=\"app_id\")\n",
        "pred_df = pred_df.merge(embedding_df, how=\"left\", on=\"app_id\")\n",
        "pred_df = pred_df.merge(tag_df, how=\"left\", on=\"app_id\")\n",
        "\n",
        "pred_df = pred_df.dropna()"
      ],
      "id": "3619e9e4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X = pred_df.drop(columns=[\"concurrent_users_yesterday\"])\n",
        "X = X.drop(columns=[\"app_id\"])\n",
        "X_features = X.columns.copy()\n",
        "\n",
        "y = pred_df[\"concurrent_users_yesterday\"]"
      ],
      "id": "5dd2ddfb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***Preprocessing***\n"
      ],
      "id": "0ea89f86"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Initial feature selection =================\n",
        "\n",
        "# Remove features with near-zero variance\n",
        "vt = VarianceThreshold(threshold=0.01)\n",
        "X = vt.fit_transform(X)\n",
        "vt_feature_names = X_features[vt.get_support()]\n",
        "\n",
        "def remove_correlated_features(df, threshold=0.95):\n",
        "    corr_matrix = df.corr().abs()\n",
        "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
        "    to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
        "    return df.drop(columns=to_drop)\n",
        "X = pd.DataFrame(X, columns=vt_feature_names)\n",
        "X = remove_correlated_features(X)\n",
        "corr_feature_names = X.columns.copy()\n",
        "\n",
        "# Split ======================================\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# More feature selection =====================\n",
        "\n",
        "# Perform feature seleciton with lasso CV\n",
        "# lasso = ElasticNetCV(cv=5, alphas=np.logspace(-4, 1, 50), max_iter=10_000).fit(X_train, y_train)\n",
        "# selected_features = X.columns[lasso.coef_ != 0]\n",
        "# print(selected_features)\n",
        "# X_train = X_train[:, lasso.coef_ != 0]\n",
        "# X_test = X_test[:, lasso.coef_ != 0]\n",
        "\n",
        "# Select top most informative features (via mutual info regression)\n",
        "k = 200\n",
        "selector = SelectKBest(mutual_info_regression, k=k)\n",
        "X_train = selector.fit_transform(X_train, y_train)\n",
        "X_test = selector.transform(X_test)\n",
        "selected_feature_names = corr_feature_names[selector.get_support()]\n",
        "\n",
        "# Scaling ====================================\n",
        "\n",
        "# scaler = StandardScaler()\n",
        "# X_train = scaler.fit_transform(X_train)\n",
        "# X_test = scaler.transform(X_test)\n",
        "\n",
        "# Target Log transform =======================\n",
        "\n",
        "# Concurrent users is highly skewed, apply log transform\n",
        "y_train_log = np.log1p(y_train)\n",
        "y_test_log = np.log1p(y_test)"
      ],
      "id": "a84ac0b8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***Modeling and Evaluation***\n"
      ],
      "id": "a8620ae4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "reg = lgb.LGBMRegressor(\n",
        "    # n_estimators=500,\n",
        "    # max_depth=6,\n",
        "    # num_leaves=31,\n",
        "    # subsample=0.8,\n",
        "    # colsample_bytree=0.8,\n",
        "    random_state=42\n",
        ")\n",
        "reg.fit(X_train, y_train_log)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred_log = reg.predict(X_test)\n",
        "y_pred = np.expm1(y_pred_log) # Convert to normal scale"
      ],
      "id": "0acd54d0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Evaluate performance\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"MAE: {mae:.4f}\")\n",
        "print(f\"RMSE: {rmse:.4f}\")\n",
        "print(f\"R-Squared: {r2:.4f}\")\n",
        "\n",
        "shap_n = 200 # Sample size\n",
        "df = pd.DataFrame(X_test[:shap_n], columns=selected_feature_names)\n",
        "\n",
        "explainer = shap.TreeExplainer(reg)\n",
        "shap_values = explainer.shap_values(df)\n",
        "shap.summary_plot(shap_values, df)"
      ],
      "id": "5adf6a50",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> After much tuning and tinkering with different preprocessing approaches, the model afforded R^2 values of approximately 0; inclusion of various features offered no better results than a trivial model. The SHAP values indicate price, tags such as multiplayer, and structural image features would be more useful than others, but this means little given the weakness of the model. To summarize, the following methods were tested:\n",
        "\n",
        "* Models\n",
        "    * Support vector machines (SVR)\n",
        "    * Random forest\n",
        "    * Gradient boosting (LightGBM)\n",
        "* Feature selection\n",
        "    * Lasso\n",
        "    * ElasticNet\n",
        "    * Selection based on mutual info regression\n",
        "    * Variance thresholding\n",
        "    * Correlation thresholding\n",
        "* Preprocessing\n",
        "    * Standard scaling (for SVM)\n",
        "    * Target log transform\n",
        "* Explainability\n",
        "    * SVM coefficient magnitudes\n",
        "    * Permutation importance\n",
        "    * SHAP values\n",
        "\n",
        "> While not exhaustive, this search indicates that these features are not quite enough for a meaningful result. Realistically, the number of concurrent users for this particular snapshot in time would likely be elucidated by time series data, information about social media presence, economic context, and plenty more not included in this dataset. Still, a more exhaustive search and extensive processing of multimodal data could afford the surprising relationships I was originally hoping to stumble upon. In lieu of that, let's try a simple model with some of the more promising features.\n"
      ],
      "id": "942020ef"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Define features and target\n",
        "X = pred_df.loc[:, [\"price_euros\", \"Great Soundtrack\", \"Indie\", \"pca_2\", \"release_day\"]]\n",
        "y = pred_df[\"concurrent_users_yesterday\"]\n",
        "\n",
        "# Split into train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a simple linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"MSE       :\", mse)\n",
        "print(\"R-Squared :\", r2)\n",
        "print(\"\\nCoefficients\")\n",
        "# Show coefficients with feature names\n",
        "for feature, coef in zip(X.columns, model.coef_):\n",
        "    print(f\"{feature}: {coef:.4f}\")"
      ],
      "id": "e95ab23d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> A simple linear regression model affords similarly poor results and little may be gleaned from the coefficients here as a result. The supposed relevance of release day of the month and negative impact of Great Soundtrack is surprising insofar as one would be willing to ignore this.\n",
        "\n",
        "---\n",
        "\n",
        "## Review Scores\n",
        "\n",
        "> Another metric of a games success is its review scores. In this case, this refers to aggregated review scores from users on the Steam platform itself. User reviews certainly differ from critic reviews, and there are many interesting questions that could be explored when armed with user review text as well. That said, this section is more modestly scoped; this particular dataset doesn't include this information and this will be a simpler effort at prediction to mirror the concurrent user exploration.\n",
        "\n",
        "### Distribution\n"
      ],
      "id": "6abdf5a4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "y = top_ids.merge(reviews, on=\"app_id\")[\"review_score\"]\n",
        "\n",
        "fig, axes = plt.subplots(2, 1, figsize=(7, 5))\n",
        "\n",
        "# Original scale\n",
        "sns.histplot(y, bins=50, kde=True, ax=axes[0])\n",
        "axes[0].set_title(\"Review Score Distribution\")\n",
        "axes[0].set_xlabel(\"Review Score\")\n",
        "axes[0].set_ylabel(\"Frequency\")\n",
        "\n",
        "# Log-transformed\n",
        "sns.histplot(np.log1p(y), bins=50, kde=True, color=\"orange\", ax=axes[1])\n",
        "axes[1].set_title(\"Log-Transformed Review Score\")\n",
        "axes[1].set_xlabel(\"log1p(Review Score)\")\n",
        "axes[1].set_ylabel(\"Frequency\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "c024197c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> This distribution isn't skewed in the same way concurrent users is, but it is skewed toward generally positive scores. A large spike is observed around 8 and smaller spikes are seen for scores of 5 and 6.\n",
        "\n",
        "### Prediction\n",
        "\n",
        "***Prepare Data***\n"
      ],
      "id": "918597a4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "reviews_df = top_ids.merge(reviews, on=\"app_id\", how=\"left\")\n",
        "\n",
        "# Filter out games without enough reviews for a score\n",
        "reviews_df = reviews_df[reviews_df[\"total\"] >= 10.0]\n",
        "\n",
        "reviews_df = reviews_df.loc[:, [\"app_id\", \"review_score\"]]\n",
        "\n",
        "# ==================================\n",
        "\n",
        "features = [\"app_id\", \"price_euros\", \"date_missing\", \"release_year\", \"release_month\", \"release_day\"]\n",
        "\n",
        "pred_df = top_ids.merge(games_df.loc[:, features], how=\"left\", on=\"app_id\")\n",
        "\n",
        "pred_df = reviews_df.merge(pred_df, on=\"app_id\", how=\"right\")\n",
        "\n",
        "# Add image, text, and tag features\n",
        "pred_df = pred_df.merge(visual_features_df.reset_index().rename({\"index\": \"app_id\"}, axis=1),\n",
        "              how=\"left\", on=\"app_id\")\n",
        "pred_df = pred_df.merge(embedding_df, how=\"left\", on=\"app_id\")\n",
        "pred_df = pred_df.merge(tag_df, how=\"left\", on=\"app_id\")\n",
        "\n",
        "pred_df = pred_df.dropna()"
      ],
      "id": "627e8f26",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X = pred_df.drop(columns=[\"review_score\"])\n",
        "X = X.drop(columns=[\"app_id\"])\n",
        "X_features = X.columns\n",
        "\n",
        "y = pred_df[\"review_score\"]"
      ],
      "id": "8531e5e0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***Preprocessing***\n"
      ],
      "id": "e8bdd324"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Initial feature selection =================\n",
        "\n",
        "# Remove features with near-zero variance\n",
        "vt = VarianceThreshold(threshold=0.01)\n",
        "X = vt.fit_transform(X)\n",
        "vt_feature_names = X_features[vt.get_support()]\n",
        "\n",
        "X = pd.DataFrame(X, columns=vt_feature_names)\n",
        "X = remove_correlated_features(X)\n",
        "corr_feature_names = X.columns.copy()\n",
        "\n",
        "# Split ======================================\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# More feature selection =====================\n",
        "\n",
        "# Select top most informative features (via mutual info regression)\n",
        "k = 200\n",
        "selector = SelectKBest(mutual_info_regression, k=k)\n",
        "X_train = selector.fit_transform(X_train, y_train)\n",
        "X_test = selector.transform(X_test)\n",
        "selected_feature_names = corr_feature_names[selector.get_support()]"
      ],
      "id": "4b0a9fb3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***Modeling and Evaluation***\n"
      ],
      "id": "a0615f8b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "reg = lgb.LGBMRegressor(\n",
        "    # n_estimators=500,\n",
        "    # max_depth=6,\n",
        "    # num_leaves=31,\n",
        "    # subsample=0.8,\n",
        "    # colsample_bytree=0.8,\n",
        "    random_state=42\n",
        ")\n",
        "reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = reg.predict(X_test)"
      ],
      "id": "4cdfcbf6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Evaluate performance\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"MAE: {mae:.4f}\")\n",
        "print(f\"RMSE: {rmse:.4f}\")\n",
        "print(f\"R-Squared: {r2:.4f}\")\n",
        "\n",
        "shap_n = 200 # Sample size\n",
        "df = pd.DataFrame(X_test[:shap_n], columns=selected_feature_names)\n",
        "\n",
        "explainer = shap.TreeExplainer(reg)\n",
        "shap_values = explainer.shap_values(df)\n",
        "shap.summary_plot(shap_values, df)"
      ],
      "id": "f26bc7e5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> This model affords marginally better results than the concurrent user predictions, but with an R-squared of slightly under 0.2 the model is still nothing special. Given the distribution of review scores, any model that generally predicts about a 7 or 8 could do similarly well. It is somewhat notable that the SHAP values indicate that the model deemed some features similarly important when predicting review score as when predicting concurrent user counts. Price is unsurprising in this regard, but there is indication that games with some of the tags in this plot and with header images including some of the structural patterns found via PCA are more important than others.\n",
        "\n",
        "---\n",
        "\n",
        "## Discussion\n",
        "\n",
        "The results of modeling concurrent user counts and review scores with the derived features proved disappointing, but there are a few possible explanations that indicate where one should look instead to better predict such metrics.\n",
        "\n",
        "As mentioned, modeling would likely benefit greatly from time series data as well as greater context ranging from details about the frequency and magnitude of sales to broad economic and geopolitical context. This is not to mention the lack of direct information about gameplay, publishers, or other details that could make this sort of exploration more fruitful.\n",
        "\n",
        "However, questions about surprising and hidden relationships between metrics of success and multimodal data being experimented with here could be answered with a deeper dive into image processing and NLP; the lack of results could just indicate the necessity of digging further into textual data like user reviews or promotional materials. Additionally, further efforts to mitigate the limitations discussed earlier could help. For a start, working with more games despite lower recent concurrent user counts could help with review scores in particular.\n",
        "\n",
        "Regarding the impact of such models (especially more successful ones), there are questions about their utility and longevity. On one hand, if certain ways of structuring promotional material or emphasizing certain features/genres were reliably found to influence success, game development and marketing could very well be pushed in directions indicated to be more promising. This sort of exploration is clearly pursued for this reason. However, various trends, surprise successes, and surprise failures indicate the dynamism of a fundamentally creative and evolving industry.\n",
        "\n",
        "Consequently, such questions to some degree join many others in facing the ever-present challenges inherent to moneyball. Another way to take such work is honing in on the creativity of development and tackling questions like why certain types of games pervade or how greater circumstances like COVID affect what games define the zeitgeist.\n"
      ],
      "id": "1e21c7c9"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "C:\\Users\\eric\\Documents\\A_OREGON\\25_winter\\cs670_datascience\\project\\cs670_project\\venv\\share\\jupyter\\kernels\\python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}